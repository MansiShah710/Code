{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Mansi Mrugen Shah\n",
    "### NetID: ws2865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a word level sequence to sequence model for English to Marathi.\n",
    "Word level models can learn syntactically and grammatically correct sentences and are more robust than character-based ones.Word level models are more memory intensive than character level models and cannot learnt out of vocabulary words. But, requires lesser training than character level models.\n",
    "\n",
    "In this model, I have used loss function as categorical crossentropy, adam optimizer and output function as softmax. I have achieved 61.5% validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zX1OUuANot7g",
    "outputId": "c82fc428-b326-4619-cc2c-dd17fa0f9bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and cleaning: remove punctuations, digits and convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2srVPPooyN_"
   },
   "outputs": [],
   "source": [
    "lines= pd.read_table('/content/mar.txt', names=['eng', 'mar', 'na'])\n",
    "lines = lines.drop(columns = ['na'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRSF1xqZoy4W"
   },
   "outputs": [],
   "source": [
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.mar=lines.mar.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKmSYcJHo3uu"
   },
   "outputs": [],
   "source": [
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.mar=lines.mar.apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJRsrPglo6Ac"
   },
   "outputs": [],
   "source": [
    "\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.mar=lines.mar.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2NTREp4o6hv"
   },
   "outputs": [],
   "source": [
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.mar=lines.mar.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.mar=lines.mar.apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9v-vpCko9Rg"
   },
   "outputs": [],
   "source": [
    "# Add start and end tokens to target sequences\n",
    "lines.mar = lines.mar.apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzXFZc3ao_MS"
   },
   "outputs": [],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of marathi\n",
    "all_marathi_words=set()\n",
    "for mar in lines.mar:\n",
    "    for word in mar.split():\n",
    "        if word not in all_marathi_words:\n",
    "            all_marathi_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4TVUJIPPpEZU",
    "outputId": "267c7a2c-b34f-44ba-95c5-b4069c9e2ed4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of source sequence\n",
    "length_list=[]\n",
    "for l in lines.eng:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(length_list)\n",
    "max_length_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cALxm0nxpGSW",
    "outputId": "8ce77355-e930-46cb-e583-17d1c8191799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Length of target sequence\n",
    "length_list=[]\n",
    "for l in lines.mar:\n",
    "    length_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(length_list)\n",
    "max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ozgTDlKTpG-G",
    "outputId": "922b9ab5-95c5-4466-c50a-127baee9023f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5762, 13566)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_marathi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_marathi_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "k0CxhLJbpJLH",
    "outputId": "0b1ce4ea-236f-476c-998c-45c14526e3d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5763"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens\n",
    "num_encoder_tokens += 1 \n",
    "num_encoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_oF-oJTpLVV"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-zWn2OM6pNVT"
   },
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "em_WOBDbpPNv",
    "outputId": "1eefcb55-263a-47dd-ca1b-d418602d1558"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>mar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>i can win</td>\n",
       "      <td>START_ मी जिंकू शकतो _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>thats my dress</td>\n",
       "      <td>START_ तो माझा ड्रेस आहे _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>where did you study</td>\n",
       "      <td>START_ कुठे अभ्यास केलास _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>i was the only woman</td>\n",
       "      <td>START_ मी एकटीच महिला होती _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21310</th>\n",
       "      <td>tom sold his gun to mary</td>\n",
       "      <td>START_ टॉमने त्याची बंदूक मेरीला विकून टाकली _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16159</th>\n",
       "      <td>what can you teach me</td>\n",
       "      <td>START_ तुम्ही मला काय शिकवू शकता _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31677</th>\n",
       "      <td>he took a room at the yaesu hotel</td>\n",
       "      <td>START_ त्याने याएसु हॉटेलात एक खोली घेतली _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36367</th>\n",
       "      <td>you should have left half an hour earlier</td>\n",
       "      <td>START_ तुम्ही अर्धा तास अगोदर निघायला हवं होतं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14310</th>\n",
       "      <td>which way did tom go</td>\n",
       "      <td>START_ टॉम कोणत्या दिशेने गेला _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34096</th>\n",
       "      <td>tom answered all of marys questions</td>\n",
       "      <td>START_ टॉमने मेरीच्या सगळ्या प्रश्नांची उत्तरं...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             eng                                                mar\n",
       "278                                    i can win                          START_ मी जिंकू शकतो _END\n",
       "4587                              thats my dress                      START_ तो माझा ड्रेस आहे _END\n",
       "12332                        where did you study                      START_ कुठे अभ्यास केलास _END\n",
       "13210                       i was the only woman                    START_ मी एकटीच महिला होती _END\n",
       "21310                   tom sold his gun to mary  START_ टॉमने त्याची बंदूक मेरीला विकून टाकली _END\n",
       "16159                      what can you teach me              START_ तुम्ही मला काय शिकवू शकता _END\n",
       "31677          he took a room at the yaesu hotel     START_ त्याने याएसु हॉटेलात एक खोली घेतली _END\n",
       "36367  you should have left half an hour earlier  START_ तुम्ही अर्धा तास अगोदर निघायला हवं होतं...\n",
       "14310                       which way did tom go                START_ टॉम कोणत्या दिशेने गेला _END\n",
       "34096        tom answered all of marys questions  START_ टॉमने मेरीच्या सगळ्या प्रश्नांची उत्तरं..."
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8EaKRx-XpRJZ",
    "outputId": "466dc25d-30a4-4be3-89a6-932fdcebb6d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36761,), (1935,))"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.mar\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCoabDlOpUfm"
   },
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ-ulGespYfy"
   },
   "outputs": [],
   "source": [
    "\n",
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "M2EJjzPSpbLF",
    "outputId": "bd5bb9d0-31c0-49da-cfae-43d0403f4688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2LyQnZs9pd4E"
   },
   "outputs": [],
   "source": [
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZO3MvEnpiaN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vGyNa7XypkQw",
    "outputId": "583d2f56-aaac-4ab4-c868-ece9202a990e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "print(val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sbYlrVmipk2C",
    "outputId": "b9de0c15-1187-42e0-f322-9adeb7da50dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.8797 - acc: 0.2112Epoch 1/150\n",
      "287/287 [==============================] - 87s 302ms/step - loss: 0.8793 - acc: 0.2113 - val_loss: 0.7966 - val_acc: 0.2475\n",
      "Epoch 2/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.7448 - acc: 0.2791Epoch 1/150\n",
      "287/287 [==============================] - 82s 285ms/step - loss: 0.7446 - acc: 0.2791 - val_loss: 0.6674 - val_acc: 0.3105\n",
      "Epoch 3/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.6510 - acc: 0.3477Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.6509 - acc: 0.3477 - val_loss: 0.6032 - val_acc: 0.3708\n",
      "Epoch 4/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.4025Epoch 1/150\n",
      "287/287 [==============================] - 82s 285ms/step - loss: 0.5764 - acc: 0.4026 - val_loss: 0.5503 - val_acc: 0.4143\n",
      "Epoch 5/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.5144 - acc: 0.4525Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.5143 - acc: 0.4526 - val_loss: 0.5160 - val_acc: 0.4432\n",
      "Epoch 6/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.4627 - acc: 0.4956Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.4627 - acc: 0.4957 - val_loss: 0.4830 - val_acc: 0.4748\n",
      "Epoch 7/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.4203 - acc: 0.5348Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.4202 - acc: 0.5349 - val_loss: 0.4577 - val_acc: 0.5036\n",
      "Epoch 8/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.5692Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.3842 - acc: 0.5692 - val_loss: 0.4431 - val_acc: 0.5189\n",
      "Epoch 9/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.3532 - acc: 0.5995Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.3531 - acc: 0.5996 - val_loss: 0.4514 - val_acc: 0.5395\n",
      "Epoch 10/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.6273Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.3260 - acc: 0.6272 - val_loss: 0.4119 - val_acc: 0.5513\n",
      "Epoch 11/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.3022 - acc: 0.6524Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.3021 - acc: 0.6525 - val_loss: 0.3966 - val_acc: 0.5655\n",
      "Epoch 12/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2813 - acc: 0.6739Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.2813 - acc: 0.6740 - val_loss: 0.3874 - val_acc: 0.5745\n",
      "Epoch 13/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2631 - acc: 0.6944Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2631 - acc: 0.6944 - val_loss: 0.3819 - val_acc: 0.5768\n",
      "Epoch 14/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2471 - acc: 0.7127Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2472 - acc: 0.7127 - val_loss: 0.3736 - val_acc: 0.5832\n",
      "Epoch 15/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.7282Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2334 - acc: 0.7283 - val_loss: 0.3664 - val_acc: 0.5907\n",
      "Epoch 16/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.7427Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2211 - acc: 0.7427 - val_loss: 0.3680 - val_acc: 0.5918\n",
      "Epoch 17/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2105 - acc: 0.7552Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2105 - acc: 0.7553 - val_loss: 0.3866 - val_acc: 0.5972\n",
      "Epoch 18/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.7667Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.2004 - acc: 0.7668 - val_loss: 0.3613 - val_acc: 0.5997\n",
      "Epoch 19/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.7776Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1912 - acc: 0.7775 - val_loss: 0.3550 - val_acc: 0.6008\n",
      "Epoch 20/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1822 - acc: 0.7871Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.1822 - acc: 0.7871 - val_loss: 0.3549 - val_acc: 0.6054\n",
      "Epoch 21/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1742 - acc: 0.7960Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.1742 - acc: 0.7960 - val_loss: 0.3565 - val_acc: 0.6043\n",
      "Epoch 22/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1670 - acc: 0.8044Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.1669 - acc: 0.8044 - val_loss: 0.3530 - val_acc: 0.6020\n",
      "Epoch 23/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.8111Epoch 1/150\n",
      "287/287 [==============================] - 80s 280ms/step - loss: 0.1602 - acc: 0.8110 - val_loss: 0.3471 - val_acc: 0.6079\n",
      "Epoch 24/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.8182Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.1538 - acc: 0.8182 - val_loss: 0.3532 - val_acc: 0.6072\n",
      "Epoch 25/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8245Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1482 - acc: 0.8245 - val_loss: 0.3738 - val_acc: 0.6088\n",
      "Epoch 26/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.8305Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1428 - acc: 0.8306 - val_loss: 0.3521 - val_acc: 0.6078\n",
      "Epoch 27/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.8361Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1380 - acc: 0.8361 - val_loss: 0.3474 - val_acc: 0.6121\n",
      "Epoch 28/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1330 - acc: 0.8410Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1330 - acc: 0.8410 - val_loss: 0.3473 - val_acc: 0.6134\n",
      "Epoch 29/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.8455Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.1287 - acc: 0.8455 - val_loss: 0.3553 - val_acc: 0.6053\n",
      "Epoch 30/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.8501Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1243 - acc: 0.8501 - val_loss: 0.3521 - val_acc: 0.6071\n",
      "Epoch 31/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1204 - acc: 0.8546Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1203 - acc: 0.8547 - val_loss: 0.3460 - val_acc: 0.6153\n",
      "Epoch 32/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.8585Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1167 - acc: 0.8584 - val_loss: 0.3516 - val_acc: 0.6099\n",
      "Epoch 33/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.8622Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1132 - acc: 0.8622 - val_loss: 0.3736 - val_acc: 0.6131\n",
      "Epoch 34/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1097 - acc: 0.8661Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.1097 - acc: 0.8662 - val_loss: 0.3514 - val_acc: 0.6122\n",
      "Epoch 35/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1066 - acc: 0.8692Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.1066 - acc: 0.8693 - val_loss: 0.3488 - val_acc: 0.6130\n",
      "Epoch 36/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.8720Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.1036 - acc: 0.8721 - val_loss: 0.3494 - val_acc: 0.6125\n",
      "Epoch 37/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.8749Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.1007 - acc: 0.8750 - val_loss: 0.3573 - val_acc: 0.6086\n",
      "Epoch 38/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.8779Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0980 - acc: 0.8779 - val_loss: 0.3536 - val_acc: 0.6082\n",
      "Epoch 39/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0953 - acc: 0.8808Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.0953 - acc: 0.8807 - val_loss: 0.3508 - val_acc: 0.6128\n",
      "Epoch 40/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.8830Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0929 - acc: 0.8829 - val_loss: 0.3564 - val_acc: 0.6043\n",
      "Epoch 41/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.8859Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0907 - acc: 0.8858 - val_loss: 0.3780 - val_acc: 0.6095\n",
      "Epoch 42/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.8879Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0885 - acc: 0.8879 - val_loss: 0.3565 - val_acc: 0.6051\n",
      "Epoch 43/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0866 - acc: 0.8894Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0866 - acc: 0.8894 - val_loss: 0.3538 - val_acc: 0.6103\n",
      "Epoch 44/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0843 - acc: 0.8923Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0844 - acc: 0.8923 - val_loss: 0.3565 - val_acc: 0.6084\n",
      "Epoch 45/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0823 - acc: 0.8945Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0824 - acc: 0.8945 - val_loss: 0.3623 - val_acc: 0.6069\n",
      "Epoch 46/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.8959Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0807 - acc: 0.8959 - val_loss: 0.3591 - val_acc: 0.6055\n",
      "Epoch 47/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0790 - acc: 0.8979Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0790 - acc: 0.8979 - val_loss: 0.3552 - val_acc: 0.6110\n",
      "Epoch 48/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.8994Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.0774 - acc: 0.8994 - val_loss: 0.3613 - val_acc: 0.6071\n",
      "Epoch 49/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9008Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0761 - acc: 0.9008 - val_loss: 0.3837 - val_acc: 0.6084\n",
      "Epoch 50/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.9021Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0746 - acc: 0.9021 - val_loss: 0.3608 - val_acc: 0.6064\n",
      "Epoch 51/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9041Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0733 - acc: 0.9040 - val_loss: 0.3589 - val_acc: 0.6085\n",
      "Epoch 52/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9057Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.0721 - acc: 0.9055 - val_loss: 0.3592 - val_acc: 0.6124\n",
      "Epoch 53/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9072Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0707 - acc: 0.9072 - val_loss: 0.3650 - val_acc: 0.6030\n",
      "Epoch 54/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9087Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0698 - acc: 0.9087 - val_loss: 0.3645 - val_acc: 0.6034\n",
      "Epoch 55/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9093Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.0687 - acc: 0.9093 - val_loss: 0.3589 - val_acc: 0.6096\n",
      "Epoch 56/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9110Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0676 - acc: 0.9110 - val_loss: 0.3656 - val_acc: 0.6027\n",
      "Epoch 57/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9121Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0668 - acc: 0.9121 - val_loss: 0.3900 - val_acc: 0.6025\n",
      "Epoch 58/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9133Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0657 - acc: 0.9134 - val_loss: 0.3684 - val_acc: 0.6020\n",
      "Epoch 59/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9143Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0651 - acc: 0.9143 - val_loss: 0.3642 - val_acc: 0.6037\n",
      "Epoch 60/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9153Epoch 1/150\n",
      "287/287 [==============================] - 82s 285ms/step - loss: 0.0642 - acc: 0.9154 - val_loss: 0.3661 - val_acc: 0.6039\n",
      "Epoch 61/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9162Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0635 - acc: 0.9162 - val_loss: 0.3733 - val_acc: 0.5981\n",
      "Epoch 62/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9175Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0627 - acc: 0.9174 - val_loss: 0.3703 - val_acc: 0.5995\n",
      "Epoch 63/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9183Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0621 - acc: 0.9184 - val_loss: 0.3671 - val_acc: 0.5979\n",
      "Epoch 64/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9190Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0615 - acc: 0.9190 - val_loss: 0.3745 - val_acc: 0.5957\n",
      "Epoch 65/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9199Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0610 - acc: 0.9199 - val_loss: 0.3976 - val_acc: 0.5966\n",
      "Epoch 66/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9208Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0604 - acc: 0.9208 - val_loss: 0.3734 - val_acc: 0.5966\n",
      "Epoch 67/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9219Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0597 - acc: 0.9219 - val_loss: 0.3699 - val_acc: 0.5977\n",
      "Epoch 68/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9230Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0591 - acc: 0.9230 - val_loss: 0.3728 - val_acc: 0.5985\n",
      "Epoch 69/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9237Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0585 - acc: 0.9237 - val_loss: 0.3783 - val_acc: 0.5920\n",
      "Epoch 70/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9243Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0580 - acc: 0.9243 - val_loss: 0.3759 - val_acc: 0.5965\n",
      "Epoch 71/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9254Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0574 - acc: 0.9254 - val_loss: 0.3720 - val_acc: 0.5957\n",
      "Epoch 72/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9257Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0571 - acc: 0.9257 - val_loss: 0.3776 - val_acc: 0.5926\n",
      "Epoch 73/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9267Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0568 - acc: 0.9267 - val_loss: 0.4017 - val_acc: 0.5932\n",
      "Epoch 74/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9273Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0562 - acc: 0.9273 - val_loss: 0.3786 - val_acc: 0.5911\n",
      "Epoch 75/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9276Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0560 - acc: 0.9276 - val_loss: 0.3758 - val_acc: 0.5973\n",
      "Epoch 76/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9285Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0556 - acc: 0.9284 - val_loss: 0.3778 - val_acc: 0.5952\n",
      "Epoch 77/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9291Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0553 - acc: 0.9291 - val_loss: 0.3843 - val_acc: 0.5927\n",
      "Epoch 78/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9301Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0547 - acc: 0.9302 - val_loss: 0.3803 - val_acc: 0.5914\n",
      "Epoch 79/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9302Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0546 - acc: 0.9303 - val_loss: 0.3765 - val_acc: 0.5953\n",
      "Epoch 80/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9311Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0543 - acc: 0.9311 - val_loss: 0.3826 - val_acc: 0.5905\n",
      "Epoch 81/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9314Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0540 - acc: 0.9315 - val_loss: 0.4054 - val_acc: 0.5928\n",
      "Epoch 82/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9321Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0538 - acc: 0.9321 - val_loss: 0.3826 - val_acc: 0.5915\n",
      "Epoch 83/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9330Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0534 - acc: 0.9330 - val_loss: 0.3798 - val_acc: 0.5948\n",
      "Epoch 84/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9329Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0532 - acc: 0.9329 - val_loss: 0.3817 - val_acc: 0.5936\n",
      "Epoch 85/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9335Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0531 - acc: 0.9335 - val_loss: 0.3882 - val_acc: 0.5911\n",
      "Epoch 86/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9335Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0529 - acc: 0.9335 - val_loss: 0.3858 - val_acc: 0.5906\n",
      "Epoch 87/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9344Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0527 - acc: 0.9344 - val_loss: 0.3828 - val_acc: 0.5909\n",
      "Epoch 88/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9346Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0525 - acc: 0.9346 - val_loss: 0.3887 - val_acc: 0.5903\n",
      "Epoch 89/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9350Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0524 - acc: 0.9351 - val_loss: 0.4133 - val_acc: 0.5887\n",
      "Epoch 90/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9352Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0523 - acc: 0.9352 - val_loss: 0.3902 - val_acc: 0.5871\n",
      "Epoch 91/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9354Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0523 - acc: 0.9354 - val_loss: 0.3854 - val_acc: 0.5885\n",
      "Epoch 92/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9356Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0521 - acc: 0.9356 - val_loss: 0.3885 - val_acc: 0.5890\n",
      "Epoch 93/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9365Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0518 - acc: 0.9365 - val_loss: 0.3926 - val_acc: 0.5866\n",
      "Epoch 94/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9362Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9362 - val_loss: 0.3893 - val_acc: 0.5871\n",
      "Epoch 95/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9365Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0517 - acc: 0.9366 - val_loss: 0.3864 - val_acc: 0.5901\n",
      "Epoch 96/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9371Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0517 - acc: 0.9371 - val_loss: 0.3932 - val_acc: 0.5856\n",
      "Epoch 97/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9369Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9369 - val_loss: 0.4176 - val_acc: 0.5873\n",
      "Epoch 98/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9376Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0515 - acc: 0.9377 - val_loss: 0.3925 - val_acc: 0.5870\n",
      "Epoch 99/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9372Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0515 - acc: 0.9372 - val_loss: 0.3886 - val_acc: 0.5892\n",
      "Epoch 100/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9374Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0518 - acc: 0.9374 - val_loss: 0.3916 - val_acc: 0.5933\n",
      "Epoch 101/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9372Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9372 - val_loss: 0.3974 - val_acc: 0.5888\n",
      "Epoch 102/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9379Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9379 - val_loss: 0.3931 - val_acc: 0.5902\n",
      "Epoch 103/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9376Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0517 - acc: 0.9376 - val_loss: 0.3880 - val_acc: 0.5922\n",
      "Epoch 104/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9381Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0518 - acc: 0.9380 - val_loss: 0.3959 - val_acc: 0.5856\n",
      "Epoch 105/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9379Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0516 - acc: 0.9378 - val_loss: 0.4193 - val_acc: 0.5870\n",
      "Epoch 106/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9382Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0516 - acc: 0.9382 - val_loss: 0.3938 - val_acc: 0.5856\n",
      "Epoch 107/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9384Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0516 - acc: 0.9384 - val_loss: 0.3903 - val_acc: 0.5892\n",
      "Epoch 108/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9382Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0516 - acc: 0.9382 - val_loss: 0.3927 - val_acc: 0.5878\n",
      "Epoch 109/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9387Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0516 - acc: 0.9387 - val_loss: 0.3994 - val_acc: 0.5843\n",
      "Epoch 110/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9385Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9385 - val_loss: 0.3957 - val_acc: 0.5878\n",
      "Epoch 111/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9388Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0516 - acc: 0.9389 - val_loss: 0.3928 - val_acc: 0.5865\n",
      "Epoch 112/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9392Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0516 - acc: 0.9393 - val_loss: 0.4009 - val_acc: 0.5832\n",
      "Epoch 113/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9391Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0517 - acc: 0.9391 - val_loss: 0.4242 - val_acc: 0.5862\n",
      "Epoch 114/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9392Epoch 1/150\n",
      "287/287 [==============================] - 82s 284ms/step - loss: 0.0518 - acc: 0.9392 - val_loss: 0.3978 - val_acc: 0.5873\n",
      "Epoch 115/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9389Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0520 - acc: 0.9389 - val_loss: 0.3959 - val_acc: 0.5841\n",
      "Epoch 116/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9390Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0520 - acc: 0.9390 - val_loss: 0.3963 - val_acc: 0.5839\n",
      "Epoch 117/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9394Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0520 - acc: 0.9395 - val_loss: 0.4027 - val_acc: 0.5858\n",
      "Epoch 118/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9392Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0520 - acc: 0.9393 - val_loss: 0.3997 - val_acc: 0.5837\n",
      "Epoch 119/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9393Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0522 - acc: 0.9393 - val_loss: 0.3952 - val_acc: 0.5853\n",
      "Epoch 120/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9394Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0523 - acc: 0.9394 - val_loss: 0.4035 - val_acc: 0.5830\n",
      "Epoch 121/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9393Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0523 - acc: 0.9394 - val_loss: 0.4265 - val_acc: 0.5839\n",
      "Epoch 122/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9396Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0525 - acc: 0.9396 - val_loss: 0.4021 - val_acc: 0.5854\n",
      "Epoch 123/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9391Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0528 - acc: 0.9391 - val_loss: 0.3977 - val_acc: 0.5855\n",
      "Epoch 124/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9392Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0529 - acc: 0.9392 - val_loss: 0.4009 - val_acc: 0.5842\n",
      "Epoch 125/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9390Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0533 - acc: 0.9390 - val_loss: 0.4079 - val_acc: 0.5788\n",
      "Epoch 126/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9387Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0535 - acc: 0.9387 - val_loss: 0.4024 - val_acc: 0.5825\n",
      "Epoch 127/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9388Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0534 - acc: 0.9388 - val_loss: 0.3992 - val_acc: 0.5823\n",
      "Epoch 128/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9385Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0537 - acc: 0.9385 - val_loss: 0.4047 - val_acc: 0.5798\n",
      "Epoch 129/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9389Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0538 - acc: 0.9389 - val_loss: 0.4313 - val_acc: 0.5773\n",
      "Epoch 130/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9388Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0539 - acc: 0.9388 - val_loss: 0.4057 - val_acc: 0.5775\n",
      "Epoch 131/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9386Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0541 - acc: 0.9386 - val_loss: 0.4010 - val_acc: 0.5830\n",
      "Epoch 132/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9388Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0542 - acc: 0.9388 - val_loss: 0.4015 - val_acc: 0.5822\n",
      "Epoch 133/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9387Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0544 - acc: 0.9388 - val_loss: 0.4061 - val_acc: 0.5828\n",
      "Epoch 134/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9384Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0545 - acc: 0.9384 - val_loss: 0.4047 - val_acc: 0.5813\n",
      "Epoch 135/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9381Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0551 - acc: 0.9381 - val_loss: 0.3997 - val_acc: 0.5836\n",
      "Epoch 136/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9385Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0551 - acc: 0.9386 - val_loss: 0.4065 - val_acc: 0.5786\n",
      "Epoch 137/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9384Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0551 - acc: 0.9384 - val_loss: 0.4302 - val_acc: 0.5812\n",
      "Epoch 138/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9380Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0555 - acc: 0.9380 - val_loss: 0.4067 - val_acc: 0.5768\n",
      "Epoch 139/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9378Epoch 1/150\n",
      "287/287 [==============================] - 81s 281ms/step - loss: 0.0555 - acc: 0.9377 - val_loss: 0.4018 - val_acc: 0.5825\n",
      "Epoch 140/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9379Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0557 - acc: 0.9379 - val_loss: 0.4019 - val_acc: 0.5814\n",
      "Epoch 141/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9376Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0561 - acc: 0.9376 - val_loss: 0.4082 - val_acc: 0.5793\n",
      "Epoch 142/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9378Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0560 - acc: 0.9378 - val_loss: 0.4062 - val_acc: 0.5805\n",
      "Epoch 143/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9377Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0562 - acc: 0.9376 - val_loss: 0.4015 - val_acc: 0.5818\n",
      "Epoch 144/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9376Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0564 - acc: 0.9376 - val_loss: 0.4053 - val_acc: 0.5806\n",
      "Epoch 145/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9377Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0564 - acc: 0.9377 - val_loss: 0.4303 - val_acc: 0.5807\n",
      "Epoch 146/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9357Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0578 - acc: 0.9358 - val_loss: 0.4076 - val_acc: 0.5800\n",
      "Epoch 147/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9309Epoch 1/150\n",
      "287/287 [==============================] - 81s 282ms/step - loss: 0.0603 - acc: 0.9310 - val_loss: 0.4017 - val_acc: 0.5835\n",
      "Epoch 148/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9359Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0581 - acc: 0.9359 - val_loss: 0.4026 - val_acc: 0.5853\n",
      "Epoch 149/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0582 - acc: 0.9356Epoch 1/150\n",
      "287/287 [==============================] - 81s 284ms/step - loss: 0.0582 - acc: 0.9357 - val_loss: 0.4117 - val_acc: 0.5776\n",
      "Epoch 150/150\n",
      "286/287 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9354Epoch 1/150\n",
      "287/287 [==============================] - 81s 283ms/step - loss: 0.0584 - acc: 0.9354 - val_loss: 0.4063 - val_acc: 0.5818\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hd7UsN0DVKdM",
    "outputId": "bf36d603-877e-4c7a-d267-f7b8486657f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy of validation:  0.6153016\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum accuracy of validation: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCskZqENpm_V"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOJP38rVpqmM"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IHL1Kg_ptuX"
   },
   "outputs": [],
   "source": [
    "\n",
    "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H0i4DCSRpv21",
    "outputId": "c5535818-5027-4135-cb4d-85bde2bf58c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i had fun here\n",
      "Actual Marathi Translation:  मला इथे मजा आली \n",
      "Predicted Marathi Translation:  मी इथे मजा आली \n",
      "\n",
      "\n",
      "Input English sentence: this book has been translated into more than fifty languages\n",
      "Actual Marathi Translation:  या पुस्तकाचा पन्नासपेक्षा जास्त भाषांमध्ये अनुवाद केला गेला आहे \n",
      "Predicted Marathi Translation:  या पुस्तकात इंग्रजीत केलंस \n",
      "\n",
      "\n",
      "Input English sentence: i have seen that girl before\n",
      "Actual Marathi Translation:  मी त्या मुलीला याआधी पाहिलं आहे \n",
      "Predicted Marathi Translation:  मी त्या जे त्या मला विकत बघितली \n",
      "\n",
      "\n",
      "Input English sentence: tom and i danced with each other\n",
      "Actual Marathi Translation:  मी आणि टॉम एकमेकांसोबत नाचलो \n",
      "Predicted Marathi Translation:  टॉम आणि मी काल एकमेकांना यायला सांगितलं \n",
      "\n",
      "\n",
      "Input English sentence: dont tell anyone this\n",
      "Actual Marathi Translation:  असं कोणाला सांगू नका \n",
      "Predicted Marathi Translation:  असं कोणाला सांगू नकोस \n",
      "\n",
      "\n",
      "Input English sentence: hes already left\n",
      "Actual Marathi Translation:  ते यापुर्वीच निघालेयत \n",
      "Predicted Marathi Translation:  तो आधीच निघाला \n",
      "\n",
      "\n",
      "Input English sentence: at last the baby fell asleep\n",
      "Actual Marathi Translation:  शेवटी बाळ झोपून गेलं \n",
      "Predicted Marathi Translation:  शेवटी बाळाला झोप लागली \n",
      "\n",
      "\n",
      "Input English sentence: this isnt what i asked for\n",
      "Actual Marathi Translation:  मी काय हे मागितलं नव्हतं \n",
      "Predicted Marathi Translation:  मी हे मागितलं नव्हतं \n",
      "\n",
      "\n",
      "Input English sentence: tom is an actor\n",
      "Actual Marathi Translation:  टॉम अभिनेता आहे \n",
      "Predicted Marathi Translation:  टॉम एक अभिनेता आहे \n",
      "\n",
      "\n",
      "Input English sentence: i speak english\n",
      "Actual Marathi Translation:  मी इंग्रजी बोलतो \n",
      "Predicted Marathi Translation:  मी इंग्रजी बोलते \n",
      "\n",
      "\n",
      "Input English sentence: besides being a doctor he was a very famous novelist\n",
      "Actual Marathi Translation:  डॉक्टर असल्याशिवाय तो एक अतिशय प्रसिद्ध कादंबरीकार होता \n",
      "Predicted Marathi Translation:  आपल्याला एक प्रसिद्ध जो मूर्ख होते \n",
      "\n",
      "\n",
      "Input English sentence: they dug a grave\n",
      "Actual Marathi Translation:  त्यांनी एक कबर खोदली \n",
      "Predicted Marathi Translation:  ते एक महान कर \n",
      "\n",
      "\n",
      "Input English sentence: she is always losing her handkerchief\n",
      "Actual Marathi Translation:  ती नेहमीच तिचा रुमाल हरवत असते \n",
      "Predicted Marathi Translation:  तिने नेहमीच तिचा सँडविच मोठा करण्यात \n",
      "\n",
      "\n",
      "Input English sentence: look at me with your books closed\n",
      "Actual Marathi Translation:  पुस्तकं बंद करून माझ्याकडे पाहा \n",
      "Predicted Marathi Translation:  ही तुझ्या तुला दिली \n",
      "\n",
      "\n",
      "Input English sentence: i went there to meet him\n",
      "Actual Marathi Translation:  मी त्यांना भेटायला तिथे गेले \n",
      "Predicted Marathi Translation:  मी त्याला भेटायला तिथे गेले \n",
      "\n",
      "\n",
      "Input English sentence: he has gone to britain\n",
      "Actual Marathi Translation:  ते ब्रिटनला गेले आहेत \n",
      "Predicted Marathi Translation:  तो गेला आहे \n",
      "\n",
      "\n",
      "Input English sentence: has he failed again\n",
      "Actual Marathi Translation:  तो परत निष्फळ झाला आहे का \n",
      "Predicted Marathi Translation:  तो परत नापास झाला आहे का \n",
      "\n",
      "\n",
      "Input English sentence: wheres tom sitting\n",
      "Actual Marathi Translation:  टॉम कुठे बसलाय \n",
      "Predicted Marathi Translation:  टॉम कुठे राहतो \n",
      "\n",
      "\n",
      "Input English sentence: tom is watching tv right now\n",
      "Actual Marathi Translation:  टॉम यावेळी टीव्ही बघत आहे \n",
      "Predicted Marathi Translation:  टॉम टीव्ही ताबडतोब टीव्ही बघत आहे \n",
      "\n",
      "\n",
      "Input English sentence: wheres my wife\n",
      "Actual Marathi Translation:  माझी पत्नी कुठे आहे \n",
      "Predicted Marathi Translation:  माझी पत्नी कुठे आहे \n",
      "\n",
      "\n",
      "Input English sentence: i dont believe in coincidences\n",
      "Actual Marathi Translation:  माझा योगायोगांवर विश्वास नाही \n",
      "Predicted Marathi Translation:  माझा विश्वास नाहीये \n",
      "\n",
      "\n",
      "Input English sentence: whats the use of worrying\n",
      "Actual Marathi Translation:  चिंता करून काय करणार \n",
      "Predicted Marathi Translation:  चिंता करण्यात काय उपयोग \n",
      "\n",
      "\n",
      "Input English sentence: stop yelling\n",
      "Actual Marathi Translation:  ओरडणं बंद कर \n",
      "Predicted Marathi Translation:  ओरडणं बंद करा \n",
      "\n",
      "\n",
      "Input English sentence: will this help them\n",
      "Actual Marathi Translation:  ह्याने त्यांची मदत होईल का \n",
      "Predicted Marathi Translation:  ही माझी मदत करू शकतो \n",
      "\n",
      "\n",
      "Input English sentence: where do you watch television\n",
      "Actual Marathi Translation:  तू टीव्ही कुठे बघतोस \n",
      "Predicted Marathi Translation:  तुम्ही टीव्ही कुठे बघता \n",
      "\n",
      "\n",
      "Input English sentence: it wasnt easy\n",
      "Actual Marathi Translation:  सोपं नव्हतं \n",
      "Predicted Marathi Translation:  ते काय सोपं नव्हतं \n",
      "\n",
      "\n",
      "Input English sentence: look at me\n",
      "Actual Marathi Translation:  माझ्याकडे बघा \n",
      "Predicted Marathi Translation:  माझ्याकडे बघ \n",
      "\n",
      "\n",
      "Input English sentence: where is your wife\n",
      "Actual Marathi Translation:  तुझी बायको कुठे आहे \n",
      "Predicted Marathi Translation:  तुझी पत्नी कुठेय \n",
      "\n",
      "\n",
      "Input English sentence: you have lots of phones\n",
      "Actual Marathi Translation:  तुझ्याकडे भरपूर फोन आहेत \n",
      "Predicted Marathi Translation:  तुमच्याकडे भरपूर फोन आहेत \n",
      "\n",
      "\n",
      "Input English sentence: brush your teeth every day\n",
      "Actual Marathi Translation:  दात दररोज घासा \n",
      "Predicted Marathi Translation:  दररोज दात काय करण्यात आली \n",
      "\n",
      "\n",
      "Input English sentence: it was her wish to go to paris\n",
      "Actual Marathi Translation:  पॅरिसला जायची इच्छा तिची होती \n",
      "Predicted Marathi Translation:  तिचं पत्र वाचतोय \n",
      "\n",
      "\n",
      "Input English sentence: theyre coming to our house we arent going to their house\n",
      "Actual Marathi Translation:  ते आमच्या घरी येताहेत आम्ही त्यांच्या घरी जात नाही आहोत \n",
      "Predicted Marathi Translation:  ते आपल्या घरी घरी तुम्हाला येताहेत तो ये गेल्या \n",
      "\n",
      "\n",
      "Input English sentence: today isnt my birthday\n",
      "Actual Marathi Translation:  आज माझा वाढदिवस नाहीये \n",
      "Predicted Marathi Translation:  आज मी माझा सर्वात चांगला होत आहे \n",
      "\n",
      "\n",
      "Input English sentence: tom came on monday\n",
      "Actual Marathi Translation:  टॉम सोमवारी आला \n",
      "Predicted Marathi Translation:  टॉम सोमवारी आला \n",
      "\n",
      "\n",
      "Input English sentence: i can do it with one hand\n",
      "Actual Marathi Translation:  मी तर एका हाताने करू शकतो \n",
      "Predicted Marathi Translation:  मी एका हाताने करू शकते \n",
      "\n",
      "\n",
      "Input English sentence: lets meet at 630\n",
      "Actual Marathi Translation:  ६३० ला भेटू या \n",
      "Predicted Marathi Translation:  तुमचा बोलू नकोस \n",
      "\n",
      "\n",
      "Input English sentence: ill decide\n",
      "Actual Marathi Translation:  मी ठरवेन \n",
      "Predicted Marathi Translation:  मी निर्णय घेईन \n",
      "\n",
      "\n",
      "Input English sentence: tom hasnt washed his hair for two weeks\n",
      "Actual Marathi Translation:  टॉमने दोन आठवड्यांमध्ये आपले केस धुतले नाहीयेत \n",
      "Predicted Marathi Translation:  टॉमने त्याच्या मी गेम टॉम घरी एकटा राहतो \n",
      "\n",
      "\n",
      "Input English sentence: tom hasnt called me\n",
      "Actual Marathi Translation:  टॉमने मला फोन केला नाहीये \n",
      "Predicted Marathi Translation:  टॉमने मला बोलवलं नाहीये \n",
      "\n",
      "\n",
      "Input English sentence: i play the french horn\n",
      "Actual Marathi Translation:  मी फ्रेंच हॉर्न वाजवतो \n",
      "Predicted Marathi Translation:  मी फ्रेंच घेऊन गेला \n",
      "\n",
      "\n",
      "Input English sentence: do tom and mary know\n",
      "Actual Marathi Translation:  टॉम आणि मेरीला माहीत आहे का \n",
      "Predicted Marathi Translation:  टॉम मेरीला माहीत आहे का \n",
      "\n",
      "\n",
      "Input English sentence: as soon as you leave the station turn left\n",
      "Actual Marathi Translation:  स्टेशन सोडल्याबरोबर डावीकडे वळा \n",
      "Predicted Marathi Translation:  स्टेशन तू थंड आहेस \n",
      "\n",
      "\n",
      "Input English sentence: is there a difference\n",
      "Actual Marathi Translation:  फरक आहे का \n",
      "Predicted Marathi Translation:  काही फरक आहे का \n",
      "\n",
      "\n",
      "Input English sentence: tom rarely wears a hat\n",
      "Actual Marathi Translation:  टॉम क्वचितच एखादी टोपी घालतो \n",
      "Predicted Marathi Translation:  टॉम एक टोपी वारला \n",
      "\n",
      "\n",
      "Input English sentence: you play the guitar very well\n",
      "Actual Marathi Translation:  तुम्ही गिटार अगदी बर्‍यापैकी वाजवता \n",
      "Predicted Marathi Translation:  तू गिटार अगदी बर्‍यापैकी वाजवतोस \n",
      "\n",
      "\n",
      "Input English sentence: i work every day except saturday\n",
      "Actual Marathi Translation:  शनिवार सोडल्यास मी दररोज काम करतो \n",
      "Predicted Marathi Translation:  मी दर वर्षी कामाला आहे \n",
      "\n",
      "\n",
      "Input English sentence: i saw a mouse\n",
      "Actual Marathi Translation:  मी एक माऊस बघितला \n",
      "Predicted Marathi Translation:  मी एक उंदीर पाहिला \n",
      "\n",
      "\n",
      "Input English sentence: i sat in front of the fan\n",
      "Actual Marathi Translation:  मी फॅनसमोर बसले \n",
      "Predicted Marathi Translation:  मी हा बसलो \n",
      "\n",
      "\n",
      "Input English sentence: how old is this tv\n",
      "Actual Marathi Translation:  हा टीव्ही किती जुना आहे \n",
      "Predicted Marathi Translation:  टॉम टीव्ही खूप टीव्ही बघायला बोलतो \n",
      "\n",
      "\n",
      "Input English sentence: whose fault is it\n",
      "Actual Marathi Translation:  ती कोणाची चूक आहे \n",
      "Predicted Marathi Translation:  कोणाची चूक आहे \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dd in range(0,50):\n",
    "  k+=1\n",
    "  (input_seq, actual_output), _ = next(val_gen)\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "  print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "  print('Actual Marathi Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "  print('Predicted Marathi Translation:', decoded_sentence[:-4])\n",
    "  print('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
