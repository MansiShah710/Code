{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmShtN1R6u0l"
   },
   "source": [
    "### Name: Mansi Mrugen Shah\n",
    "### NetID: ws2865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-rUmQUA6u0l"
   },
   "source": [
    "### Build a word level sequence to sequence model for English to Marathi.(using GloVe embedding)\n",
    "\n",
    "In this model, I have used GloVe word embedding. The GloVe stands for Global Vectors and it is a set pretrained word embeddings. This helped me in gaining an accuracy of 73%. The loss function used in this model is categorical crossentropy, adam optimizer and output function as softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxZN3LWp6u0m"
   },
   "source": [
    "####  Import the required libraries and configure values for different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5qFAkcqrKYXk",
    "outputId": "3540e8ae-68a0-493f-9ab7-a3ac854333f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "%tensorflow_version 1.x\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e6vur8w17BZF",
    "outputId": "2df2b39d-5a52-4545-d98b-26f9656d7a94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38696, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of sentences\n",
    "lines= pd.read_table('/content/mar.txt', names=['eng', 'mar', 'na'])\n",
    "lines = lines.drop(columns = ['na'])\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhARdVBSKZSH"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 20000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tbp-GkuC6u0t"
   },
   "source": [
    "### Load the dataset and clean the data by removing punctuations, digits and converting to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79p6UXb56u0t"
   },
   "source": [
    "The seq2seq architecture is an encoder-decoder architecture which consists of two LSTM networks:the encoder LSTM and the decoder LSTM.\n",
    "The input to the encoder LSTM is the sentence in English; the input to the decoder LSTM is the sentence in Marathi with a start-of-sentence token. The output is the actual target sentence with an end-of-sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "coVhITSf6u0u",
    "outputId": "54a976b6-e47a-4b2a-9790-09de4cd04e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 20000\n",
      "num samples output: 20000\n",
      "num samples output input: 20000\n"
     ]
    }
   ],
   "source": [
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "exclude = set(string.punctuation)\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "count = 0\n",
    "for line in open(r'/content/mar.txt', encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output, c = line.rstrip().split('\\t')\n",
    "    input_sentence = input_sentence.lower()\n",
    "    output = output.lower()\n",
    "    input_sentence = re.sub(\"'\", '', input_sentence)\n",
    "    input_sentence = re.sub(\",\", ' COMMA', input_sentence)\n",
    "    output = re.sub(\"'\", '', output)\n",
    "    output = re.sub(\",\", ' COMMA', output)\n",
    "    input_sentence = ''.join(x for x in input_sentence if x not in exclude)\n",
    "    output = ''.join(x for x in output if x not in exclude)\n",
    "    input_sentence = input_sentence.translate(remove_digits)\n",
    "    output = output.translate(remove_digits)\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "UC0wYHpJKo8N",
    "outputId": "02ab3c4d-695b-4df6-ca99-0945d78953cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sit here\n",
      "इथे बस <eos>\n",
      "<sos> इथे बस\n"
     ]
    }
   ],
   "source": [
    "# randomly print a sentence\n",
    "print(input_sentences[182])\n",
    "print(output_sentences[182])\n",
    "print(output_sentences_inputs[182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dc05baA-6u00"
   },
   "source": [
    "### Tokenize the input sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CaIcisp0Kvg4",
    "outputId": "cc7d0d72-35da-4763-bc38-189cc197e192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3003\n",
      "Length of longest sentence in input: 7\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLO6LvXk6u03"
   },
   "source": [
    "### Tokenize the output sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "INVRZNH3K0Qu",
    "outputId": "3eeca6b0-ae75-4dc1-c0b1-070a14d06b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 6385\n",
      "Length of longest sentence in the output: 10\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YerLlVLP6u06"
   },
   "source": [
    "### Apply  padding to the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "K201tbUIK4Jr",
    "outputId": "2dabd9e8-15a0-4cdd-d79e-fa2d5a2b38c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (20000, 7)\n",
      "encoder_input_sequences[182]: [  0   0   0   0   0 288  34]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[182]:\", encoder_input_sequences[182])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4qKcuG0uK7kT",
    "outputId": "974f983d-4586-45ff-b9dd-227d1db48e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"can\"])\n",
    "print(word2idx_inputs[\"take\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply padding to decoder outputs and the decoder inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "J2WZse0FK-FN",
    "outputId": "422fe338-6a7e-4a94-ac10-83526e953d1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 10)\n",
      "decoder_input_sequences[182]: [  2  34 249   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[182]:\", decoder_input_sequences[182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe word embedding - loading the GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB-xFdH8LCYk"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(r'/content/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matrix where the row number will represent the integer value for the word and the columns will correspond to the dimensions of the word. This matrix will contain the word embeddings for the words in our input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSCz_cqILFsj"
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding layer for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI23_YdhLPeH"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the empty output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNC49KHoLgmQ"
   },
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MhOGXamFLno9",
    "outputId": "9a78d7ae-d4b2-4fe6-d9ab-15e576544d73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10, 6386)"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create one-hot encoded output as the final layer of the model will be a dense layer, therefore we need the outputs in the form of one-hot encoded vectors, since we will be using softmax activation function at the dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5y0rcChLrCO"
   },
   "outputs": [],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the encoder. The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YBwTKL3NyP5"
   },
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an 'sos' token appended at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufx0sfzqN__5"
   },
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True, dropout = 0.3)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The output from the decoder LSTM is passed through a dense layer to predict decoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0rVdkqRLsE9"
   },
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EiXi06TOEjS"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using the fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "f4m19JXcOJTD",
    "outputId": "e89d4bcf-8c3f-4f04-ca48-a9d04a7e26d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "18000/18000 [==============================] - 17s 936us/step - loss: 2.4945 - acc: 0.6407 - val_loss: 2.7697 - val_acc: 0.5935\n",
      "Epoch 2/20\n",
      "18000/18000 [==============================] - 14s 796us/step - loss: 1.8719 - acc: 0.7056 - val_loss: 2.4269 - val_acc: 0.6353\n",
      "Epoch 3/20\n",
      "18000/18000 [==============================] - 14s 798us/step - loss: 1.5802 - acc: 0.7424 - val_loss: 2.2582 - val_acc: 0.6600\n",
      "Epoch 4/20\n",
      "18000/18000 [==============================] - 14s 790us/step - loss: 1.3739 - acc: 0.7687 - val_loss: 2.0935 - val_acc: 0.6810\n",
      "Epoch 5/20\n",
      "18000/18000 [==============================] - 14s 793us/step - loss: 1.2219 - acc: 0.7902 - val_loss: 2.0206 - val_acc: 0.6897\n",
      "Epoch 6/20\n",
      "18000/18000 [==============================] - 14s 792us/step - loss: 1.1027 - acc: 0.8084 - val_loss: 1.9481 - val_acc: 0.6982\n",
      "Epoch 7/20\n",
      "18000/18000 [==============================] - 14s 793us/step - loss: 1.0049 - acc: 0.8233 - val_loss: 1.9036 - val_acc: 0.7096\n",
      "Epoch 8/20\n",
      "18000/18000 [==============================] - 14s 790us/step - loss: 0.9235 - acc: 0.8363 - val_loss: 1.8727 - val_acc: 0.7110\n",
      "Epoch 9/20\n",
      "18000/18000 [==============================] - 14s 784us/step - loss: 0.8548 - acc: 0.8469 - val_loss: 1.8401 - val_acc: 0.7148\n",
      "Epoch 10/20\n",
      "18000/18000 [==============================] - 14s 788us/step - loss: 0.7939 - acc: 0.8557 - val_loss: 1.8459 - val_acc: 0.7177\n",
      "Epoch 11/20\n",
      "18000/18000 [==============================] - 14s 778us/step - loss: 0.7465 - acc: 0.8638 - val_loss: 1.8164 - val_acc: 0.7230\n",
      "Epoch 12/20\n",
      "18000/18000 [==============================] - 14s 785us/step - loss: 0.7038 - acc: 0.8702 - val_loss: 1.8123 - val_acc: 0.7253\n",
      "Epoch 13/20\n",
      "18000/18000 [==============================] - 14s 779us/step - loss: 0.6691 - acc: 0.8757 - val_loss: 1.8075 - val_acc: 0.7279\n",
      "Epoch 14/20\n",
      "18000/18000 [==============================] - 14s 790us/step - loss: 0.6388 - acc: 0.8812 - val_loss: 1.8013 - val_acc: 0.7279\n",
      "Epoch 15/20\n",
      "18000/18000 [==============================] - 14s 783us/step - loss: 0.6087 - acc: 0.8861 - val_loss: 1.8018 - val_acc: 0.7277\n",
      "Epoch 16/20\n",
      "18000/18000 [==============================] - 14s 785us/step - loss: 0.5856 - acc: 0.8893 - val_loss: 1.8014 - val_acc: 0.7300\n",
      "Epoch 17/20\n",
      "18000/18000 [==============================] - 14s 789us/step - loss: 0.5683 - acc: 0.8924 - val_loss: 1.8124 - val_acc: 0.7288\n",
      "Epoch 18/20\n",
      "18000/18000 [==============================] - 14s 786us/step - loss: 0.5531 - acc: 0.8955 - val_loss: 1.8078 - val_acc: 0.7317\n",
      "Epoch 19/20\n",
      "18000/18000 [==============================] - 14s 782us/step - loss: 0.5397 - acc: 0.8975 - val_loss: 1.8255 - val_acc: 0.7291\n",
      "Epoch 20/20\n",
      "18000/18000 [==============================] - 14s 787us/step - loss: 0.5256 - acc: 0.8997 - val_loss: 1.8323 - val_acc: 0.7305\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the maximum validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3NGAf1QZ_4XK",
    "outputId": "606962d0-9cae-4e45-f8b4-464f26675456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy of validation:  0.731650004863739\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum accuracy of validation: \", max(history.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZLqpizKQFDk"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kaSXrAsNQIoi"
   },
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COekZ0BpQKrl"
   },
   "outputs": [],
   "source": [
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5s6heF3iQNAi"
   },
   "outputs": [],
   "source": [
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To make predictions, the decoder output is passed through the dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mqeHwXhZQRQR"
   },
   "outputs": [],
   "source": [
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dos3QLp1QVzr"
   },
   "outputs": [],
   "source": [
    "# updated decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "In the tokenization steps, we converted words to integers. The outputs we obtain from the decoder will be integers. Since our goal is to get the output as words in the Marathi language, we need to convert these integer outputs back to words. To achieve this, we will create new dictionaries for both inputs and outputs where the keys will be the integers and the corresponding values will be the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYkwG9ASQXjW"
   },
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### translate_sentence() method will accept an input-padded sequence English sentence (in the integer form) and will return the translated Marathi sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tot5Wuk8Qbpv"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Efj78TZlQhkx",
    "outputId": "03c6c179-6775-443c-9444-8f9e2c1cb1b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: we are good friends\n",
      "Actual: आम्ही चांगले मित्र आहोत <eos>\n",
      "Response: आम्ही चांगले मित्र आहोत\n",
      "-\n",
      "Input: we love our children\n",
      "Actual: आपलं आपल्या मुलांवर प्रेम आहे <eos>\n",
      "Response: आपलं आपल्या मुलांवर प्रेम आहे\n",
      "-\n",
      "Input: she made me hurry\n",
      "Actual: त्यांनी मला घाई करायला लावली <eos>\n",
      "Response: तिने मला घाई करायला लावली\n",
      "-\n",
      "Input: did you manage to sleep\n",
      "Actual: झोपायला जमलं का <eos>\n",
      "Response: झोप झोपायला का\n",
      "-\n",
      "Input: do you know who won\n",
      "Actual: कोण जिंकलं हे तुम्हाला माहीत आहे का <eos>\n",
      "Response: कोण जिंकलं हे तुम्हाला माहीत आहे का\n",
      "-\n",
      "Input: call me tonight\n",
      "Actual: मला आज रात्री बोलव <eos>\n",
      "Response: मला आज रात्री फोन करा\n",
      "-\n",
      "Input: were you responsible\n",
      "Actual: तू जबाबदार होतास का <eos>\n",
      "Response: तू जबाबदार होतीस का\n",
      "-\n",
      "Input: science is fun\n",
      "Actual: विज्ञानात मजा येते <eos>\n",
      "Response: मजा येते का\n",
      "-\n",
      "Input: im ready to go\n",
      "Actual: मी जायला तयार आहे <eos>\n",
      "Response: मी जायला तयार आहे\n",
      "-\n",
      "Input: she called him\n",
      "Actual: तिने त्यांना फोन केला <eos>\n",
      "Response: तिने त्यांना फोन केला\n",
      "-\n",
      "Input: use your head\n",
      "Actual: डोकं वापर <eos>\n",
      "Response: डोकं वापर\n",
      "-\n",
      "Input: this is not salt\n",
      "Actual: हे मीठ नाही <eos>\n",
      "Response: हे नाही आहे\n",
      "-\n",
      "Input: this isnt for sale\n",
      "Actual: हे विकण्यासाठी नाही आहे <eos>\n",
      "Response: हे नाही आहे\n",
      "-\n",
      "Input: give me the file\n",
      "Actual: मला फाइल दे <eos>\n",
      "Response: मला फाइल दे\n",
      "-\n",
      "Input: why dont you eat meat\n",
      "Actual: तुम्ही मांस का नाही खात <eos>\n",
      "Response: तू इथे का नाही\n",
      "-\n",
      "Input: he took off his hat\n",
      "Actual: त्यांनी त्यांची टोपी काढली <eos>\n",
      "Response: त्याने त्याची टोपी काढली\n",
      "-\n",
      "Input: how do you like my gown\n",
      "Actual: तुला माझा गाऊन कसा वाटला <eos>\n",
      "Response: तुला मला कसं आवडतो\n",
      "-\n",
      "Input: did you see it\n",
      "Actual: तू बघितलंस का <eos>\n",
      "Response: तुम्ही ते बघितलं का\n",
      "-\n",
      "Input: bring wine\n",
      "Actual: वाईन आणा <eos>\n",
      "Response: वाईन आण\n",
      "-\n",
      "Input: tom was screaming\n",
      "Actual: टॉम किंचाळत होता <eos>\n",
      "Response: टॉम होता\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "  i = np.random.choice(len(input_sentences))\n",
    "  input_seq = encoder_input_sequences[i:i+1]\n",
    "  translation = translate_sentence(input_seq)\n",
    "  print('-')\n",
    "  print('Input:', input_sentences[i])\n",
    "  print('Actual:', output_sentences[i])\n",
    "  print('Response:', translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Pretrained_word_level.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
